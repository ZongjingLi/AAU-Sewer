{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from model import *\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAUSewer(Dataset):\n",
    "    def __init__(self,split = \"train\",type = \"real\"):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        \n",
    "        self.train_data = []\n",
    "        self.labels = []\n",
    "\n",
    "        path = os.path.join(dataDir, \"{}_{}.h5\".format(\"{}ing_pointcloud_hdf5\".format(split), type))\n",
    "        print(path)\n",
    "        with h5py.File(path, 'r') as hdf:          \n",
    "            if split == \"train\":\n",
    "                partitions = [\"Training\"]\n",
    "            else:\n",
    "                partitions = [\"Testing\"]\n",
    "\n",
    "            for partition in partitions:\n",
    "                self.train_data = np.asarray(hdf[f'{partition}/PointClouds'][:])\n",
    "                self.labels = np.asarray(hdf[f'{partition}/Labels'][:])\n",
    "      \n",
    "\n",
    "    def __len__(self):return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        labels = self.labels[index]\n",
    "        if labels > 1: labels = 1\n",
    "        return self.train_data[index],labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/AAU/training_pointcloud_hdf5_synthetic.h5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/AAU/training_pointcloud_hdf5_synthetic.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000002?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m PointNetCls(\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000002?line=2'>3</a>\u001b[0m \u001b[39m#model = torch.load(\"point_net.ckpt\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000002?line=4'>5</a>\u001b[0m aau_syn_train \u001b[39m=\u001b[39m  aau_syn \u001b[39m=\u001b[39m AAUSewer(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39msynthetic\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m#\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000002?line=5'>6</a>\u001b[0m aau_syn_test \u001b[39m=\u001b[39m  aau_syn \u001b[39m=\u001b[39m AAUSewer(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msynthetic\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m#\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000002?line=7'>8</a>\u001b[0m aau_real_train \u001b[39m=\u001b[39m  aau_syn \u001b[39m=\u001b[39m AAUSewer(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m#\u001b[39;00m\n",
      "\u001b[1;32m/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb Cell 2'\u001b[0m in \u001b[0;36mAAUSewer.__init__\u001b[0;34m(self, split, type)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000001?line=8'>9</a>\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataDir, \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39ming_pointcloud_hdf5\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(split), \u001b[39mtype\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(path)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000001?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39;49mFile(path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m hdf:          \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000001?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/melkor/Documents/GitHub/AAU-Sewer/output.ipynb#ch0000001?line=12'>13</a>\u001b[0m         partitions \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=524'>525</a>\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=525'>526</a>\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=526'>527</a>\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=527'>528</a>\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=528'>529</a>\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=529'>530</a>\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=530'>531</a>\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=531'>532</a>\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=532'>533</a>\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=534'>535</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=535'>536</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=223'>224</a>\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=224'>225</a>\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=225'>226</a>\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=226'>227</a>\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/melkor/miniforge3/envs/Melkor/lib/python3.9/site-packages/h5py/_hl/files.py?line=227'>228</a>\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/AAU/training_pointcloud_hdf5_synthetic.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "model = PointNetCls(2)\n",
    "#model = torch.load(\"point_net.ckpt\")\n",
    "\n",
    "aau_syn_train =  aau_syn = AAUSewer(\"train\",\"synthetic\")#\n",
    "aau_syn_test =  aau_syn = AAUSewer(\"test\",\"synthetic\")#\n",
    "\n",
    "aau_real_train =  aau_syn = AAUSewer(\"train\",\"real\")#\n",
    "aau_real_test =  aau_syn = AAUSewer(\"test\",\"real\")#\n",
    "    \n",
    "\n",
    "opt_parser = argparse.ArgumentParser()\n",
    "opt_parser.add_argument(\"--epoch\",            default = 1000)\n",
    "opt_parser.add_argument(\"--lr\",               default = 2e-4)\n",
    "opt_parser.add_argument(\"--source_batch_size\",default = 32)\n",
    "opt_parser.add_argument(\"--target_batch_size\",default = 32)\n",
    "opt_parser.add_argument(\"--warmup_constant\",  default = 0.2)\n",
    "opt_parser.add_argument(\"--confidence_threshold\",default = 0.9)\n",
    "opt_parser.add_argument(\"--tau\",               default = 0.2)\n",
    "opt_parser.add_argument(\"--batch_size\",        default = 16)\n",
    "opt = opt_parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DomainCluster(label,feature,c = 0,tau = 0.002):\n",
    "    cummulate = 0\n",
    "    size = label.shape[0]\n",
    "    for i in range(size):\n",
    "        if int(label[i]) == c:\n",
    "            contrastive_end = 0\n",
    "            for j in range(size):\n",
    "                if int(label[j]) != c:\n",
    "                    contrastive_end += torch.exp(torch.cosine_similarity(feature[i],feature[j],dim = 0) / tau)\n",
    "            \n",
    "            for k in range(size):\n",
    "                if int(label[i]) == int(label[j]) and i != j:\n",
    "                    upper = torch.cosine_similarity(feature[i],feature[k],dim = 0) / tau\n",
    "                    lower = torch.exp(torch.cosine_similarity(feature[i],feature[k],dim = 0) / tau) + contrastive_end\n",
    "                    cummulate -= upper - torch.log(lower)\n",
    "                \n",
    "    return cummulate\n",
    "\n",
    "def DomainContrast(slabel,tlabel,sfeature,tfeature,c = 0,tau = 0.002):\n",
    "    cummulate = 0\n",
    "\n",
    "    s_size = slabel.shape[0]\n",
    "    t_size = tlabel.shape[0]\n",
    "\n",
    "    for i in range(s_size):\n",
    "        if int(slabel[i]) == c:\n",
    "            contrastive_end = 0\n",
    "            for j in range(t_size):\n",
    "                if int(tlabel[j]) != c:\n",
    "                    contrastive_end += torch.exp(torch.cosine_similarity(sfeature[i],tfeature[j],dim = 0) / tau)\n",
    "            scores = []\n",
    "            for j_ in range(t_size):\n",
    "                if int(slabel[i]) == int(tlabel[j]) and i != j:\n",
    "                    scores.append(torch.cosine_similarity(sfeature[i],tfeature[j_],dim = 0).detach())\n",
    "            scores = np.array(scores)\n",
    "          \n",
    "            if len(scores) != 0:\n",
    "                \n",
    "                k = np.argmax(scores)\n",
    "\n",
    "                upper = torch.cosine_similarity(sfeature[i],tfeature[k],dim = 0) / tau\n",
    "                lower = torch.exp(torch.cosine_similarity(sfeature[i],tfeature[k],dim = 0) / tau) + contrastive_end\n",
    "                cummulate -= upper - torch.log(lower)\n",
    "    return cummulate\n",
    "\n",
    "def DomainTransfer(model,opt,source_dataset,target_dataset):\n",
    "\n",
    "    # create the Adam optimizer with lr\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = opt.lr)\n",
    "\n",
    "    # setup the dataloader for source domain and target domain\n",
    "\n",
    "    if not isinstance(source_dataset,DataLoader):\n",
    "        source_loader = DataLoader(source_dataset,shuffle = True, batch_size = opt.source_batch_size)\n",
    "    else:source_loader = source_dataset\n",
    "    if not isinstance(target_dataset,DataLoader):\n",
    "        target_loader = DataLoader(target_dataset,shuffle = True, batch_size = opt.target_batch_size)\n",
    "    else:target_loader = target_dataset\n",
    "    \n",
    "    # temperature constant tau for scaling\n",
    "    tau = opt.tau\n",
    "\n",
    "    for K in range(3): # setup 100 transfer steps\n",
    "        for _ in range(12): # get this much samples from the source and target dataloader\n",
    "            working_loss = 0\n",
    "            try: # get a batch from the source datset \n",
    "                source_data, source_label = next(dataloader_iterator)\n",
    "            except:\n",
    "                dataloader_iterator = iter(source_loader)\n",
    "                source_data, source_label = next(dataloader_iterator)\n",
    "            try: # get a batch from the target dataset\n",
    "                target_data, target_label = next(target_iterator)\n",
    "            except:\n",
    "                target_iterator = iter(target_loader)\n",
    "                target_data, _ = next(target_iterator)\n",
    "\n",
    "\n",
    "            # [Calculate the Regular Prediction Loss for Source]\n",
    "       \n",
    "            source_prediction,source_feature,_ = model(source_data.permute(0,2,1))\n",
    "            target_prediction,target_feature,_ = model(target_data.permute(0,2,1))\n",
    "\n",
    "            # calculate peudo labels for target data\n",
    "\n",
    "            target_label = np.argmax(target_prediction.cpu().detach().numpy(),1)\n",
    "\n",
    "            predict_loss = 0\n",
    "\n",
    "\n",
    "            for i in range(source_prediction.shape[0]):\n",
    "                predict_loss -= source_prediction[i][source_label[i]]\n",
    "            working_loss += predict_loss * 500\n",
    "            \n",
    "            # [Calculate Contrastive Loss Between Two Batches]\n",
    "            for clsid in range(4): # peform contrastive learning for binary classification\n",
    "                # [Source Domain Loss]\n",
    "\n",
    "                source_loss = DomainCluster(source_label,source_feature,c = clsid)\n",
    "\n",
    "                # [Target Domain Loss]\n",
    "                target_loss = DomainCluster(target_label,target_feature,c = clsid)\n",
    "\n",
    "                # [Inter Domain Loss]\n",
    "                inter_loss = DomainContrast(source_label,target_label,source_feature,target_feature,c = clsid)\n",
    "                for loss in [source_loss ,target_loss , inter_loss]:\n",
    "                    try:\n",
    "                        working_loss += loss\n",
    "                    except:pass\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            working_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(\"Working Loss:{} Predict Loss:{}\".format(working_loss,predict_loss.detach().numpy()))\n",
    "        print(\"K Iter:\",K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:384 375 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n"
     ]
    }
   ],
   "source": [
    "def eval(model,dataset):\n",
    "    if not isinstance(dataset,DataLoader): \n",
    "        dataloader = DataLoader(dataset,shuffle = True,batch_size = 4)\n",
    "    else: dataloader = dataset\n",
    "\n",
    "    pt_at = 0\n",
    "    pt_af = 0\n",
    "    pf_at = 0\n",
    "    pf_af = 0\n",
    "\n",
    "    for sample in dataloader:\n",
    "        x,label = sample\n",
    "\n",
    "        prediction,feature,_ = model(x.permute(0,2,1))\n",
    "        for i in range(label.shape[0]):\n",
    "            predict_label = np.argmax(prediction[i].detach().numpy())\n",
    "            actual_label = int(label[i])\n",
    "            if predict_label == 0:\n",
    "                if actual_label == 0:pt_at += 1\n",
    "                else:pt_af += 1\n",
    "            \n",
    "            if predict_label == 1:\n",
    "                if actual_label == 0:pf_at += 1\n",
    "                else:pf_af += 1\n",
    "    accuracy = (pt_at + pt_af) / (pt_at + pt_af + pf_at + pf_af)\n",
    "    precision = pt_at/(pt_at + pf_at)\n",
    "    recall = pt_at/(pt_at + pf_af)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall )\n",
    "    print(\"Raw:{} {} {} {}\".format(pt_at,pt_af,pf_at,pf_af))\n",
    "    print(\"Actual:{} Precision:{} Recall:{} F1:{} \".format(accuracy,\\\n",
    "                            pt_at/(pt_at + pf_at),\\\n",
    "                            pt_at/(pt_at + pf_af),\\\n",
    "                                f1))\n",
    "\n",
    "\"\"\"\n",
    "[Setup]\n",
    "\"\"\"\n",
    "\n",
    "source_dataset = torch.utils.data.ConcatDataset([aau_syn_train,aau_syn_test])\n",
    "target_dataset = torch.utils.data.ConcatDataset([aau_real_train,aau_real_test])\n",
    "\n",
    "k = 10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "eval(model,dataset = target_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(enumerate(splits.split(np.arange(len(target_dataset)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:384 375 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 1\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 2\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 3\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 4\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 5\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 6\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 7\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 8\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 9\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 10\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n"
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "eval(model,dataset = target_dataset)       \n",
    "for source_fold, (source_train_idx,source_val_idx) in enumerate(splits.split(np.arange(len(source_dataset)))):\n",
    "    target_fold, (target_train_idx,target_val_idx) = next(enumerate(splits.split(np.arange(len(target_dataset)))))\n",
    "    print('Fold {}'.format(source_fold + 1))\n",
    "\n",
    "    source_train_sampler = SubsetRandomSampler(source_train_idx)\n",
    "    source_test_sampler = SubsetRandomSampler(source_val_idx)\n",
    "    source_train_loader = DataLoader(source_dataset, batch_size=opt.batch_size, sampler=source_train_sampler)\n",
    "    source_test_loader = DataLoader(source_dataset, batch_size=opt.batch_size, sampler=source_test_sampler)\n",
    "    \n",
    "\n",
    "    target_train_sampler = SubsetRandomSampler(target_train_idx)\n",
    "    target_test_sampler = SubsetRandomSampler(target_val_idx)\n",
    "    target_train_loader = DataLoader(target_dataset, batch_size=opt.batch_size, sampler=target_train_sampler)\n",
    "    target_test_loader = DataLoader(target_dataset, batch_size=opt.batch_size, sampler=target_test_sampler)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        DomainTransfer(model,opt,source_train_loader,target_train_loader) \n",
    "        eval(model,target_test_loader)       \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e46093c771f9510c4aabf3710bfb1355e5f870a13f8c22092f45d4d23626d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Melkor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
