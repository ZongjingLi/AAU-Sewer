{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from model import *\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAUSewer(Dataset):\n",
    "    def __init__(self,split = \"train\",type = \"real\"):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        \n",
    "        self.train_data = []\n",
    "        self.labels = []\n",
    "\n",
    "        path = os.path.join(dataDir, \"{}_{}.h5\".format(\"{}ing_pointcloud_hdf5\".format(split), type))\n",
    "        print(path)\n",
    "        with h5py.File(path, 'r') as hdf:          \n",
    "            if split == \"train\":\n",
    "                partitions = [\"Training\"]\n",
    "            else:\n",
    "                partitions = [\"Testing\"]\n",
    "\n",
    "            for partition in partitions:\n",
    "                self.train_data = np.asarray(hdf[f'{partition}/PointClouds'][:])\n",
    "                self.labels = np.asarray(hdf[f'{partition}/Labels'][:])\n",
    "      \n",
    "\n",
    "    def __len__(self):return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        labels = self.labels[index]\n",
    "        if labels > 1: labels = 1\n",
    "        return self.train_data[index],labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/melkor/Desktop/datasets/AAU/training_pointcloud_hdf5_synthetic.h5\n",
      "/Users/melkor/Desktop/datasets/AAU/testing_pointcloud_hdf5_synthetic.h5\n",
      "/Users/melkor/Desktop/datasets/AAU/training_pointcloud_hdf5_real.h5\n",
      "/Users/melkor/Desktop/datasets/AAU/testing_pointcloud_hdf5_real.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "model = PointNetCls(2)\n",
    "#model = torch.load(\"point_net.ckpt\")\n",
    "\n",
    "aau_syn_train =  aau_syn = AAUSewer(\"train\",\"synthetic\")#\n",
    "aau_syn_test =  aau_syn = AAUSewer(\"test\",\"synthetic\")#\n",
    "\n",
    "aau_real_train =  aau_syn = AAUSewer(\"train\",\"real\")#\n",
    "aau_real_test =  aau_syn = AAUSewer(\"test\",\"real\")#\n",
    "    \n",
    "\n",
    "opt_parser = argparse.ArgumentParser()\n",
    "opt_parser.add_argument(\"--epoch\",            default = 1000)\n",
    "opt_parser.add_argument(\"--lr\",               default = 2e-4)\n",
    "opt_parser.add_argument(\"--source_batch_size\",default = 32)\n",
    "opt_parser.add_argument(\"--target_batch_size\",default = 32)\n",
    "opt_parser.add_argument(\"--warmup_constant\",  default = 0.2)\n",
    "opt_parser.add_argument(\"--confidence_threshold\",default = 0.9)\n",
    "opt_parser.add_argument(\"--tau\",               default = 0.2)\n",
    "opt_parser.add_argument(\"--batch_size\",        default = 16)\n",
    "opt = opt_parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DomainCluster(label,feature,c = 0,tau = 0.002):\n",
    "    cummulate = 0\n",
    "    size = label.shape[0]\n",
    "    for i in range(size):\n",
    "        if int(label[i]) == c:\n",
    "            contrastive_end = 0\n",
    "            for j in range(size):\n",
    "                if int(label[j]) != c:\n",
    "                    contrastive_end += torch.exp(torch.cosine_similarity(feature[i],feature[j],dim = 0) / tau)\n",
    "            \n",
    "            for k in range(size):\n",
    "                if int(label[i]) == int(label[j]) and i != j:\n",
    "                    upper = torch.cosine_similarity(feature[i],feature[k],dim = 0) / tau\n",
    "                    lower = torch.exp(torch.cosine_similarity(feature[i],feature[k],dim = 0) / tau) + contrastive_end\n",
    "                    cummulate -= upper - torch.log(lower)\n",
    "                \n",
    "    return cummulate\n",
    "\n",
    "def DomainContrast(slabel,tlabel,sfeature,tfeature,c = 0,tau = 0.002):\n",
    "    cummulate = 0\n",
    "\n",
    "    s_size = slabel.shape[0]\n",
    "    t_size = tlabel.shape[0]\n",
    "\n",
    "    for i in range(s_size):\n",
    "        if int(slabel[i]) == c:\n",
    "            contrastive_end = 0\n",
    "            for j in range(t_size):\n",
    "                if int(tlabel[j]) != c:\n",
    "                    contrastive_end += torch.exp(torch.cosine_similarity(sfeature[i],tfeature[j],dim = 0) / tau)\n",
    "            scores = []\n",
    "            for j_ in range(t_size):\n",
    "                if int(slabel[i]) == int(tlabel[j]) and i != j:\n",
    "                    scores.append(torch.cosine_similarity(sfeature[i],tfeature[j_],dim = 0).detach())\n",
    "            scores = np.array(scores)\n",
    "          \n",
    "            if len(scores) != 0:\n",
    "                \n",
    "                k = np.argmax(scores)\n",
    "\n",
    "                upper = torch.cosine_similarity(sfeature[i],tfeature[k],dim = 0) / tau\n",
    "                lower = torch.exp(torch.cosine_similarity(sfeature[i],tfeature[k],dim = 0) / tau) + contrastive_end\n",
    "                cummulate -= upper - torch.log(lower)\n",
    "    return cummulate\n",
    "\n",
    "def DomainTransfer(model,opt,source_dataset,target_dataset):\n",
    "\n",
    "    # create the Adam optimizer with lr\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = opt.lr)\n",
    "\n",
    "    # setup the dataloader for source domain and target domain\n",
    "\n",
    "    if not isinstance(source_dataset,DataLoader):\n",
    "        source_loader = DataLoader(source_dataset,shuffle = True, batch_size = opt.source_batch_size)\n",
    "    else:source_loader = source_dataset\n",
    "    if not isinstance(target_dataset,DataLoader):\n",
    "        target_loader = DataLoader(target_dataset,shuffle = True, batch_size = opt.target_batch_size)\n",
    "    else:target_loader = target_dataset\n",
    "    \n",
    "    # temperature constant tau for scaling\n",
    "    tau = opt.tau\n",
    "\n",
    "    for K in range(3): # setup 100 transfer steps\n",
    "        for _ in range(12): # get this much samples from the source and target dataloader\n",
    "            working_loss = 0\n",
    "            try: # get a batch from the source datset \n",
    "                source_data, source_label = next(dataloader_iterator)\n",
    "            except:\n",
    "                dataloader_iterator = iter(source_loader)\n",
    "                source_data, source_label = next(dataloader_iterator)\n",
    "            try: # get a batch from the target dataset\n",
    "                target_data, target_label = next(target_iterator)\n",
    "            except:\n",
    "                target_iterator = iter(target_loader)\n",
    "                target_data, _ = next(target_iterator)\n",
    "\n",
    "\n",
    "            # [Calculate the Regular Prediction Loss for Source]\n",
    "       \n",
    "            source_prediction,source_feature,_ = model(source_data.permute(0,2,1))\n",
    "            target_prediction,target_feature,_ = model(target_data.permute(0,2,1))\n",
    "\n",
    "            # calculate peudo labels for target data\n",
    "\n",
    "            target_label = np.argmax(target_prediction.cpu().detach().numpy(),1)\n",
    "\n",
    "            predict_loss = 0\n",
    "\n",
    "\n",
    "            for i in range(source_prediction.shape[0]):\n",
    "                predict_loss -= source_prediction[i][source_label[i]]\n",
    "            working_loss += predict_loss * 500\n",
    "            \n",
    "            # [Calculate Contrastive Loss Between Two Batches]\n",
    "            for clsid in range(4): # peform contrastive learning for binary classification\n",
    "                # [Source Domain Loss]\n",
    "\n",
    "                source_loss = DomainCluster(source_label,source_feature,c = clsid)\n",
    "\n",
    "                # [Target Domain Loss]\n",
    "                target_loss = DomainCluster(target_label,target_feature,c = clsid)\n",
    "\n",
    "                # [Inter Domain Loss]\n",
    "                inter_loss = DomainContrast(source_label,target_label,source_feature,target_feature,c = clsid)\n",
    "                for loss in [source_loss ,target_loss , inter_loss]:\n",
    "                    try:\n",
    "                        working_loss += loss\n",
    "                    except:pass\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            working_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(\"Working Loss:{} Predict Loss:{}\".format(working_loss,predict_loss.detach().numpy()))\n",
    "        print(\"K Iter:\",K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:384 375 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n"
     ]
    }
   ],
   "source": [
    "def eval(model,dataset):\n",
    "    if not isinstance(dataset,DataLoader): \n",
    "        dataloader = DataLoader(dataset,shuffle = True,batch_size = 4)\n",
    "    else: dataloader = dataset\n",
    "\n",
    "    pt_at = 0\n",
    "    pt_af = 0\n",
    "    pf_at = 0\n",
    "    pf_af = 0\n",
    "\n",
    "    for sample in dataloader:\n",
    "        x,label = sample\n",
    "\n",
    "        prediction,feature,_ = model(x.permute(0,2,1))\n",
    "        for i in range(label.shape[0]):\n",
    "            predict_label = np.argmax(prediction[i].detach().numpy())\n",
    "            actual_label = int(label[i])\n",
    "            if predict_label == 0:\n",
    "                if actual_label == 0:pt_at += 1\n",
    "                else:pt_af += 1\n",
    "            \n",
    "            if predict_label == 1:\n",
    "                if actual_label == 0:pf_at += 1\n",
    "                else:pf_af += 1\n",
    "    accuracy = (pt_at + pt_af) / (pt_at + pt_af + pf_at + pf_af)\n",
    "    precision = pt_at/(pt_at + pf_at)\n",
    "    recall = pt_at/(pt_at + pf_af)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall )\n",
    "    print(\"Raw:{} {} {} {}\".format(pt_at,pt_af,pf_at,pf_af))\n",
    "    print(\"Actual:{} Precision:{} Recall:{} F1:{} \".format(accuracy,\\\n",
    "                            pt_at/(pt_at + pf_at),\\\n",
    "                            pt_at/(pt_at + pf_af),\\\n",
    "                                f1))\n",
    "\n",
    "\"\"\"\n",
    "[Setup]\n",
    "\"\"\"\n",
    "\n",
    "source_dataset = torch.utils.data.ConcatDataset([aau_syn_train,aau_syn_test])\n",
    "target_dataset = torch.utils.data.ConcatDataset([aau_real_train,aau_real_test])\n",
    "\n",
    "k = 10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "eval(model,dataset = target_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(enumerate(splits.split(np.arange(len(target_dataset)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:384 375 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 1\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 2\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 3\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 4\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 5\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 6\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 7\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 8\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 9\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n",
      "Fold 10\n",
      "K Iter: 0\n",
      "K Iter: 1\n",
      "K Iter: 2\n",
      "Raw:37 39 0 0\n",
      "Actual:1.0 Precision:1.0 Recall:1.0 F1:1.0 \n"
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "eval(model,dataset = target_dataset)       \n",
    "for source_fold, (source_train_idx,source_val_idx) in enumerate(splits.split(np.arange(len(source_dataset)))):\n",
    "    target_fold, (target_train_idx,target_val_idx) = next(enumerate(splits.split(np.arange(len(target_dataset)))))\n",
    "    print('Fold {}'.format(source_fold + 1))\n",
    "\n",
    "    source_train_sampler = SubsetRandomSampler(source_train_idx)\n",
    "    source_test_sampler = SubsetRandomSampler(source_val_idx)\n",
    "    source_train_loader = DataLoader(source_dataset, batch_size=opt.batch_size, sampler=source_train_sampler)\n",
    "    source_test_loader = DataLoader(source_dataset, batch_size=opt.batch_size, sampler=source_test_sampler)\n",
    "    \n",
    "\n",
    "    target_train_sampler = SubsetRandomSampler(target_train_idx)\n",
    "    target_test_sampler = SubsetRandomSampler(target_val_idx)\n",
    "    target_train_loader = DataLoader(target_dataset, batch_size=opt.batch_size, sampler=target_train_sampler)\n",
    "    target_test_loader = DataLoader(target_dataset, batch_size=opt.batch_size, sampler=target_test_sampler)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        DomainTransfer(model,opt,source_train_loader,target_train_loader) \n",
    "        eval(model,target_test_loader)       \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e46093c771f9510c4aabf3710bfb1355e5f870a13f8c22092f45d4d23626d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Melkor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
